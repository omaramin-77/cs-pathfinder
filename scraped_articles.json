[
  {
    "title": "Xvantage",
    "url": "https://bit.ly/48JbJwD",
    "summary": "The overarching goal is to maximize the return on analytical talent, shifting their focus entirely from data preparation to predictive model development, which is a necessary move if the business intends to compete in an AI-driven economy.",
    "image_url": "https://www.kdnuggets.com/wp-content/uploads/pexels-todd-trapani-488382-1385056-scaled.jpg",
    "published_date": null,
    "author": null,
    "full_text": "",
    "full_html": "",
    "thumbnail": null,
    "word_count": 0,
    "scraped_at": "2025-12-17T18:52:49.551918"
  },
  {
    "title": "How to Handle Large Datasets in Python Even If You're a Beginner",
    "url": "https://www.kdnuggets.com/how-to-handle-large-datasets-in-python-even-if-youre-a-beginner",
    "summary": "You don‚Äôt need advanced skills to work with large datasets. With Python‚Äôs built-in features and libraries, you can handle large datasets without breaking a sweat even if you're a beginner.",
    "image_url": "https://www.kdnuggets.com/wp-content/uploads/bala-python-large-datasets.png",
    "published_date": null,
    "author": "https://www.facebook.com/kdnuggets",
    "full_text": "Image by Author\n#\nIntroduction\nWorking with large datasets in Python often leads to a common problem: you load your data with\nPandas\n, and your program slows to a crawl or crashes entirely. This typically occurs because you are attempting to load everything into memory simultaneously.\nMost memory issues stem from\nhow\nyou load and process data. With a handful of practical techniques, you can handle datasets much larger than your available memory.\nIn this article, you will learn seven techniques for working with large datasets efficiently in Python. We will start simply and build up, so by the end, you will know exactly which approach fits your use case.\nüîó You can find the\ncode on GitHub\n. If you‚Äôd like, you can run this\nsample data generator Python script\nto get sample CSV files and use the code snippets to process them.\n#\n1. Read Data in Chunks\nThe most beginner-friendly approach is to process your data in smaller pieces instead of loading everything at once.\nConsider a scenario where you have a large sales dataset and you want to find the total revenue. The following code demonstrates this approach:\nimport pandas as pd\n# Define chunk size (number of rows per chunk)\nchunk_size = 100000\ntotal_revenue = 0\n# Read and process the file in chunks\nfor chunk in pd.read_csv('large_sales_data.csv', chunksize=chunk_size):\n# Process each chunk\ntotal_revenue += chunk['revenue'].sum()\nprint(f\"Total Revenue: ${total_revenue:,.2f}\")\nInstead of loading all 10 million rows at once, we are loading 100,000 rows at a time. We calculate the sum for each chunk and add it to our running total. Your RAM only ever holds 100,000 rows, no matter how big the file is.\nWhen to use this\n: When you need to perform aggregations (sum, count, average) or filtering operations on large files.\n#\n2. Use Specific Columns Only\nOften, you do not need every column in your dataset. Loading only what you need can reduce memory usage significantly.\nSuppose you are analyzing customer data, but you only require age and purchase amount, rather than the numerous other columns:\nimport pandas as pd\n# Only load the columns you actually need\ncolumns_to_use = ['customer_id', 'age', 'purchase_amount']\ndf = pd.read_csv('customers.csv', usecols=columns_to_use)\n# Now work with a much lighter dataframe\naverage_purchase = df.groupby('age')['purchase_amount'].mean()\nprint(average_purchase)\nBy specifying\nusecols\n, Pandas only loads those three columns into memory. If your original file had 50 columns, you have just cut your memory usage by roughly 94%.\nWhen to use this\n: When you know exactly which columns you need before loading the data.\n#\n3. Optimize Data Types\nBy default, Pandas might use more memory than necessary. A column of integers might be stored as 64-bit when 8-bit would work fine.\nFor instance, if you are loading a dataset with product ratings (1-5 stars) and user IDs:\nimport pandas as pd\n# First, let's see the default memory usage\ndf = pd.read_csv('ratings.csv')\nprint(\"Default memory usage:\")\nprint(df.memory_usage(deep=True))\n# Now optimize the data types\ndf['rating'] = df['rating'].astype('int8')  # Ratings are 1-5, so int8 is enough\ndf['user_id'] = df['user_id'].astype('int32')  # Assuming user IDs fit in int32\nprint(\"\\nOptimized memory usage:\")\nprint(df.memory_usage(deep=True))\nBy converting the rating column from the probable\nint64\n(8 bytes per number) to\nint8\n(1 byte per number), we achieve an 8x memory reduction for that column.\nCommon conversions include:\nint64\n‚Üí\nint8\n,\nint16\n, or\nint32\n(depending on the range of numbers).\nfloat64\n‚Üí\nfloat32\n(if you do not need extreme precision).\nobject\n‚Üí\ncategory\n(for columns with repeated values).\n#\n4. Use Categorical Data Types\nWhen a column contains repeated text values (like country names or product categories), Pandas stores each value separately. The\ncategory\ndtype stores the unique values once and uses efficient codes to reference them.\nSuppose you are working with a product inventory file where the category column has only 20 unique values, but they repeat across all rows in the dataset:\nimport pandas as pd\ndf = pd.read_csv('products.csv')\n# Check memory before conversion\nprint(f\"Before: {df['category'].memory_usage(deep=True) / 1024**2:.2f} MB\")\n# Convert to category\ndf['category'] = df['category'].astype('category')\n# Check memory after conversion\nprint(f\"After: {df['category'].memory_usage(deep=True) / 1024**2:.2f} MB\")\n# It still works like normal text\nprint(df['category'].value_counts())\nThis conversion can substantially reduce memory usage for columns with low cardinality (few unique values). The column still functions similarly to standard text data: you can filter, group, and sort as usual.\nWhen to use this\n: For any text column where values repeat frequently (categories, states, countries, departments, and the like).\n#\n5. Filter While Reading\nSometimes you know you only need a subset of rows. Instead of loading everything and then filtering, you can filter during the load process.\nFor example, if you only care about transactions from the year 2024:\nimport pandas as pd\n# Read in chunks and filter\nchunk_size = 100000\nfiltered_chunks = []\nfor chunk in pd.read_csv('transactions.csv', chunksize=chunk_size):\n# Filter each chunk before storing it\nfiltered = chunk[chunk['year'] == 2024]\nfiltered_chunks.append(filtered)\n# Combine the filtered chunks\ndf_2024 = pd.concat(filtered_chunks, ignore_index=True)\nprint(f\"Loaded {len(df_2024)} rows from 2024\")\nWe are combining chunking with filtering. Each chunk is filtered before being added to our list, so we never hold the full dataset in memory, only the rows we actually want.\nWhen to use this\n: When you need only a subset of rows based on some condition.\n#\n6. Use Dask for Parallel Processing\nFor datasets that are truly massive,\nDask\nprovides a Pandas-like API but handles all the chunking and parallel processing automatically.\nHere is how you would calculate the average of a column across a huge dataset:\nimport dask.dataframe as dd\n# Read with Dask (it handles chunking automatically)\ndf = dd.read_csv('huge_dataset.csv')\n# Operations look just like pandas\nresult = df['sales'].mean()\n# Dask is lazy - compute() actually executes the calculation\naverage_sales = result.compute()\nprint(f\"Average Sales: ${average_sales:,.2f}\")\nDask does not load the entire file into memory. Instead, it creates a plan for how to process the data in chunks and executes that plan when you call\n.compute()\n. It can even use multiple CPU cores to speed up computation.\nWhen to use this\n: When your dataset is too large for Pandas, even with chunking, or when you want parallel processing without writing complex code.\n#\n7. Sample Your Data for Exploration\nWhen you are just exploring or testing code, you do not need the full dataset. Load a sample first.\nSuppose you are building a machine learning model and want to test your preprocessing pipeline. You can sample your dataset as shown:\nimport pandas as pd\n# Read just the first 50,000 rows\ndf_sample = pd.read_csv('huge_dataset.csv', nrows=50000)\n# Or read a random sample using skiprows\nimport random\nskip_rows = lambda x: x > 0 and random.random() > 0.01  # Keep ~1% of rows\ndf_random_sample = pd.read_csv('huge_dataset.csv', skiprows=skip_rows)\nprint(f\"Sample size: {len(df_random_sample)} rows\")\nThe first approach loads the first N rows, which is suitable for rapid exploration. The second approach randomly samples rows throughout the file, which is better for statistical analysis or when the file is sorted in a way that makes the top rows unrepresentative.\nWhen to use this\n: During development, testing, or exploratory analysis before running your code on the full dataset.\n#\nConclusion\nHandling large datasets does not require expert-level skills. Here is a quick summary of techniques we have discussed:\nTechnique\nWhen to use it\nChunking\nFor aggregations, filtering, and processing data you cannot fit in RAM.\nColumn selection\nWhen you need only a few columns from a wide dataset.\nData type optimization\nAlways; do this after loading to save memory.\nCategorical types\nFor text columns with repeated values (categories, states, etc.).\nFilter while reading\nWhen you need only a subset of rows.\nDask\nFor very large datasets or when you want parallel processing.\nSampling\nDuring development and exploration.\nThe first step is knowing both your data and your task\n. Most of the time, a combination of chunking and smart column selection will get you 90% of the way there.\nAs your needs grow, move to more advanced tools like Dask or consider converting your data to more efficient file formats like\nParquet\nor\nHDF5\n.\nNow go ahead and start working with those massive datasets. Happy analyzing!\nBala Priya C\nis a developer and technical writer from India. She likes working at the intersection of math, programming, data science, and content creation. Her areas of interest and expertise include DevOps, data science, and natural language processing. She enjoys reading, writing, coding, and coffee! Currently, she's working on learning and sharing her knowledge with the developer community by authoring tutorials, how-to guides, opinion pieces, and more. Bala also creates engaging resource overviews and coding tutorials.\n<= Previous post",
    "full_html": "<div class=\"single\" id=\"content\">\n\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<p><center><img alt=\"How to Handle Large Datasets in Python Even If You're a Beginner\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/bala-python-large-datasets.png\" width=\"100%\"><br>\n<font size=\"-1\">Image by Author</font></br></img></center><br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Introduction</h2>\n<p>¬†<br/>\nWorking with large datasets in Python often leads to a common problem: you load your data with <strong><a href=\"https://pandas.pydata.org/\" target=\"_blank\">Pandas</a></strong>, and your program slows to a crawl or crashes entirely. This typically occurs because you are attempting to load everything into memory simultaneously.</p><div class=\"kdnug-c367f6031db2e7a69d956bf0241008e7 kdnug-after-first-paragraph\" id=\"kdnug-c367f6031db2e7a69d956bf0241008e7\"></div>\n<p>Most memory issues stem from <em>how</em> you load and process data. With a handful of practical techniques, you can handle datasets much larger than your available memory.</p><div class=\"kdnug-42a6d56cde015b8c7423a525288d2dcf kdnug-in-content-1\" id=\"kdnug-42a6d56cde015b8c7423a525288d2dcf\" style=\"padding-bottom:180px; padding-top:20px\"></div>\n<p>In this article, you will learn seven techniques for working with large datasets efficiently in Python. We will start simply and build up, so by the end, you will know exactly which approach fits your use case.</p>\n<blockquote><p>\nüîó You can find the <a href=\"https://github.com/balapriyac/python-basics/tree/main/working-with-large-datasets\" target=\"_blank\"><strong>code on GitHub</strong></a>. If you‚Äôd like, you can run this <a href=\"https://github.com/balapriyac/python-basics/blob/main/working-with-large-datasets/sample_data_generator.py\" target=\"_blank\"><strong>sample data generator Python script</strong></a> to get sample CSV files and use the code snippets to process them.\n</p></blockquote>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>1. Read Data in Chunks</h2>\n<p>¬†<br/>\nThe most beginner-friendly approach is to process your data in smaller pieces instead of loading everything at once.</p>\n<p>Consider a scenario where you have a large sales dataset and you want to find the total revenue. The following code demonstrates this approach:</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>import pandas as pd\r\n\r\n# Define chunk size (number of rows per chunk)\r\nchunk_size = 100000\r\ntotal_revenue = 0\r\n\r\n# Read and process the file in chunks\r\nfor chunk in pd.read_csv('large_sales_data.csv', chunksize=chunk_size):\r\n    # Process each chunk\r\n    total_revenue += chunk['revenue'].sum()\r\n\r\nprint(f\"Total Revenue: ${total_revenue:,.2f}\")</code></pre>\n</div>\n<p>¬†</p>\n<p>Instead of loading all 10 million rows at once, we are loading 100,000 rows at a time. We calculate the sum for each chunk and add it to our running total. Your RAM only ever holds 100,000 rows, no matter how big the file is.</p>\n<p><strong>When to use this</strong>: When you need to perform aggregations (sum, count, average) or filtering operations on large files.<br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>2. Use Specific Columns Only</h2>\n<p>¬†<br/>\nOften, you do not need every column in your dataset. Loading only what you need can reduce memory usage significantly.</p>\n<p>Suppose you are analyzing customer data, but you only require age and purchase amount, rather than the numerous other columns:</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>import pandas as pd\r\n\r\n# Only load the columns you actually need\r\ncolumns_to_use = ['customer_id', 'age', 'purchase_amount']\r\n\r\ndf = pd.read_csv('customers.csv', usecols=columns_to_use)\r\n\r\n# Now work with a much lighter dataframe\r\naverage_purchase = df.groupby('age')['purchase_amount'].mean()\r\nprint(average_purchase)</code></pre>\n</div>\n<p>¬†</p>\n<p>By specifying <code>usecols</code>, Pandas only loads those three columns into memory. If your original file had 50 columns, you have just cut your memory usage by roughly 94%.</p>\n<p><strong>When to use this</strong>: When you know exactly which columns you need before loading the data.<br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>3. Optimize Data Types</h2>\n<p>¬†<br/>\nBy default, Pandas might use more memory than necessary. A column of integers might be stored as 64-bit when 8-bit would work fine.</p>\n<p>For instance, if you are loading a dataset with product ratings (1-5 stars) and user IDs:</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>import pandas as pd\r\n\r\n# First, let's see the default memory usage\r\ndf = pd.read_csv('ratings.csv')\r\nprint(\"Default memory usage:\")\r\nprint(df.memory_usage(deep=True))\r\n\r\n# Now optimize the data types\r\ndf['rating'] = df['rating'].astype('int8')  # Ratings are 1-5, so int8 is enough\r\ndf['user_id'] = df['user_id'].astype('int32')  # Assuming user IDs fit in int32\r\n\r\nprint(\"\\nOptimized memory usage:\")\r\nprint(df.memory_usage(deep=True))</code></pre>\n</div>\n<p>¬†</p>\n<p>By converting the rating column from the probable <code>int64</code> (8 bytes per number) to <code>int8</code> (1 byte per number), we achieve an 8x memory reduction for that column.</p>\n<p>Common conversions include:</p>\n<ul>\n<li><code>int64</code> ‚Üí <code>int8</code>, <code>int16</code>, or <code>int32</code> (depending on the range of numbers).\n<li><code>float64</code> ‚Üí <code>float32</code> (if you do not need extreme precision).\n<li><code>object</code> ‚Üí <code>category</code> (for columns with repeated values).\n</li></li></li></ul>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>4. Use Categorical Data Types</h2>\n<p>¬†<br/>\nWhen a column contains repeated text values (like country names or product categories), Pandas stores each value separately. The <code>category</code> dtype stores the unique values once and uses efficient codes to reference them.</p>\n<p>Suppose you are working with a product inventory file where the category column has only 20 unique values, but they repeat across all rows in the dataset:</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>import pandas as pd\r\n\r\ndf = pd.read_csv('products.csv')\r\n\r\n# Check memory before conversion\r\nprint(f\"Before: {df['category'].memory_usage(deep=True) / 1024**2:.2f} MB\")\r\n\r\n# Convert to category\r\ndf['category'] = df['category'].astype('category')\r\n\r\n# Check memory after conversion\r\nprint(f\"After: {df['category'].memory_usage(deep=True) / 1024**2:.2f} MB\")\r\n\r\n# It still works like normal text\r\nprint(df['category'].value_counts())</code></pre>\n</div>\n<p>¬†</p>\n<p>This conversion can substantially reduce memory usage for columns with low cardinality (few unique values). The column still functions similarly to standard text data: you can filter, group, and sort as usual.</p>\n<p><strong>When to use this</strong>: For any text column where values repeat frequently (categories, states, countries, departments, and the like).<br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>5. Filter While Reading</h2>\n<p>¬†<br/>\nSometimes you know you only need a subset of rows. Instead of loading everything and then filtering, you can filter during the load process.</p>\n<p>For example, if you only care about transactions from the year 2024:</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>import pandas as pd\r\n\r\n# Read in chunks and filter\r\nchunk_size = 100000\r\nfiltered_chunks = []\r\n\r\nfor chunk in pd.read_csv('transactions.csv', chunksize=chunk_size):\r\n    # Filter each chunk before storing it\r\n    filtered = chunk[chunk['year'] == 2024]\r\n    filtered_chunks.append(filtered)\r\n\r\n# Combine the filtered chunks\r\ndf_2024 = pd.concat(filtered_chunks, ignore_index=True)\r\n\r\nprint(f\"Loaded {len(df_2024)} rows from 2024\")</code></pre>\n</div>\n<p>¬†</p>\n<p>We are combining chunking with filtering. Each chunk is filtered before being added to our list, so we never hold the full dataset in memory, only the rows we actually want.</p>\n<p><strong>When to use this</strong>: When you need only a subset of rows based on some condition.<br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>6. Use Dask for Parallel Processing</h2>\n<p>¬†<br/>\nFor datasets that are truly massive, <strong><a href=\"https://www.dask.org/\" target=\"_blank\">Dask</a></strong> provides a Pandas-like API but handles all the chunking and parallel processing automatically.</p>\n<p>Here is how you would calculate the average of a column across a huge dataset:</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>import dask.dataframe as dd\r\n\r\n# Read with Dask (it handles chunking automatically)\r\ndf = dd.read_csv('huge_dataset.csv')\r\n\r\n# Operations look just like pandas\r\nresult = df['sales'].mean()\r\n\r\n# Dask is lazy - compute() actually executes the calculation\r\naverage_sales = result.compute()\r\n\r\nprint(f\"Average Sales: ${average_sales:,.2f}\")</code></pre>\n</div>\n<p>¬†</p>\n<p>Dask does not load the entire file into memory. Instead, it creates a plan for how to process the data in chunks and executes that plan when you call <code>.compute()</code>. It can even use multiple CPU cores to speed up computation.</p>\n<p><strong>When to use this</strong>: When your dataset is too large for Pandas, even with chunking, or when you want parallel processing without writing complex code.<br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>7. Sample Your Data for Exploration</h2>\n<p>¬†<br/>\nWhen you are just exploring or testing code, you do not need the full dataset. Load a sample first.</p>\n<p>Suppose you are building a machine learning model and want to test your preprocessing pipeline. You can sample your dataset as shown:</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>import pandas as pd\r\n\r\n# Read just the first 50,000 rows\r\ndf_sample = pd.read_csv('huge_dataset.csv', nrows=50000)\r\n\r\n# Or read a random sample using skiprows\r\nimport random\r\nskip_rows = lambda x: x &gt; 0 and random.random() &gt; 0.01  # Keep ~1% of rows\r\n\r\ndf_random_sample = pd.read_csv('huge_dataset.csv', skiprows=skip_rows)\r\n\r\nprint(f\"Sample size: {len(df_random_sample)} rows\")</code></pre>\n</div>\n<p>¬†</p>\n<p>The first approach loads the first N rows, which is suitable for rapid exploration. The second approach randomly samples rows throughout the file, which is better for statistical analysis or when the file is sorted in a way that makes the top rows unrepresentative.</p>\n<p><strong>When to use this</strong>: During development, testing, or exploratory analysis before running your code on the full dataset.<br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Conclusion</h2>\n<p>¬†<br/>\nHandling large datasets does not require expert-level skills. Here is a quick summary of techniques we have discussed:<br/>\n¬†</p>\n<table style=\"width: 100%; border-collapse: collapse; font-family: Arial, sans-serif; font-size: 14px\">\n<thead>\n<tr>\n<th style=\"padding: 12px; border: 1px solid #ddd; text-align: left\">Technique</th>\n<th style=\"padding: 12px; border: 1px solid #ddd; text-align: left\">When to use it</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\"><strong>Chunking</strong></td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">\nFor aggregations, filtering, and processing data you cannot fit in RAM.\n</td>\n</tr>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\"><strong>Column selection</strong></td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">\nWhen you need only a few columns from a wide dataset.\n</td>\n</tr>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\"><strong>Data type optimization</strong></td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">\nAlways; do this after loading to save memory.\n</td>\n</tr>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\"><strong>Categorical types</strong></td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">\nFor text columns with repeated values (categories, states, etc.).\n</td>\n</tr>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\"><strong>Filter while reading</strong></td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">\nWhen you need only a subset of rows.\n</td>\n</tr>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\"><strong>Dask</strong></td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">\nFor very large datasets or when you want parallel processing.\n</td>\n</tr>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\"><strong>Sampling</strong></td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">\nDuring development and exploration.\n</td>\n</tr>\n</tbody>\n</table>\n<p>¬†</p>\n<p><strong>The first step is knowing both your data and your task</strong>. Most of the time, a combination of chunking and smart column selection will get you 90% of the way there.</p>\n<p>As your needs grow, move to more advanced tools like Dask or consider converting your data to more efficient file formats like <strong><a href=\"https://parquet.apache.org/\" target=\"_blank\">Parquet</a></strong> or <strong><a href=\"https://www.hdfgroup.org/solutions/hdf5/\" target=\"_blank\">HDF5</a></strong>.</p>\n<p>Now go ahead and start working with those massive datasets. Happy analyzing!<br/>\n¬†<br/>\n¬†</p>\n<p><b><a href=\"https://twitter.com/balawc27\" rel=\"noopener\"><strong><a href=\"https://www.kdnuggets.com/wp-content/uploads/bala-priya-author-image-update-230821.jpg\" rel=\"noopener noreferrer\" target=\"_blank\">Bala Priya C</a></strong></a></b> is a developer and technical writer from India. She likes working at the intersection of math, programming, data science, and content creation. Her areas of interest and expertise include DevOps, data science, and natural language processing. She enjoys reading, writing, coding, and coffee! Currently, she's working on learning and sharing her knowledge with the developer community by authoring tutorials, how-to guides, opinion pieces, and more. Bala also creates engaging resource overviews and coding tutorials.</p>\n</div><br/>\n<!--<hr class=\"grey-line\"><br>\t\n  <div><h3>Our Top 5 Free Course Recommendations</h3><br>\n\t  \t</div>\t\n-->\n<div class=\"box-form\">\n<!-- Mailchimp for WordPress v4.10.9 - https://wordpress.org/plugins/mailchimp-for-wp/ --><!-- / Mailchimp for WordPress Plugin --></div>\n<!-- You can start editing here. -->\n\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<br/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/5-data-privacy-stories-from-2025-every-analyst-should-know\" rel=\"prev\">&lt;= Previous post</a></div>\n<div class=\"pagi-right\">\n</div>\n<br/><br/>\n</div>\n<!--#content end--></div>",
    "thumbnail": "https://www.kdnuggets.com/wp-content/uploads/bala-python-large-datasets.png",
    "word_count": 1438,
    "scraped_at": "2025-12-17T18:52:49.854140"
  },
  {
    "title": "5 Data Privacy Stories from 2025 Every Analyst Should Know",
    "url": "https://www.kdnuggets.com/5-data-privacy-stories-from-2025-every-analyst-should-know",
    "summary": "In this article we look at 5 specific privacy stories from 2025 that changed how analysts work day to day, from the code they write to the reports they publish.",
    "image_url": "https://www.kdnuggets.com/wp-content/uploads/kdn-olumide-5-data-privacy-stories-from-2025-analysts-should-know.png",
    "published_date": null,
    "author": "https://www.facebook.com/kdnuggets",
    "full_text": "Image by Editor\n#\nIntroduction\nIf you work with data for a living, 2025 has probably felt different. Privacy used to be something your legal team handled in a long PDF nobody read. This year, it crept straight into everyday analytics work. The rules changed, and suddenly, people who write R scripts, clean CSVs in Python, build Excel dashboards, or ship weekly reports are expected to understand how their choices affect compliance.\nThat shift didn‚Äôt happen because regulators started caring more about data. It happened because data analysis is where privacy problems actually show up. A single unlabeled AI-generated chart, an extra column left in a dataset, or a model trained on undocumented data can put a company on the wrong side of the law. And in 2025, regulators stopped giving warnings and started handing out real penalties.\nIn this article, we will take a look at five specific stories from 2025 that should matter to anyone who touches data. These aren‚Äôt abstract trends or high-level policy notes. They‚Äôre real events that changed how analysts work day to day, from the code you write to the reports you publish.\n#\n1. The EU AI Act‚Äôs First Enforcement Phase Hit Analysts Harder Than Developers\nWhen the\nEU AI Act\nofficially moved into its first enforcement phase in early 2025, most teams expected model builders and machine learning leads to feel the pressure. Instead, the first wave of compliance work landed squarely on analysts. The reason was simple: regulators focused on data inputs and documentation, not just AI model behavior.\nAcross Europe, companies were suddenly required to prove where training data came from, how it was labeled, and whether any AI-generated content inside their datasets was clearly marked. That meant analysts had to rebuild the very basics of their workflow. R notebooks needed provenance notes. Python pipelines needed metadata fields for ‚Äú\nsynthetic\nvs.\nreal\n.‚Äù Even shared Excel workbooks had to carry small disclaimers explaining whether AI was used to clean or transform the data.\nTeams also learned quickly that ‚ÄúAI transparency‚Äù is not a developer-only concept. If an analyst used\nCopilot\n,\nGemini\n, or\nChatGPT\nto write part of a query or generate a quick summary table, the output needed to be identified as AI-assisted in regulated industries. For many teams, that meant adopting a simple tagging practice, something as basic as adding a short metadata note like ‚Äú\nGenerated with AI, validated by analyst\n.‚Äù It wasn‚Äôt elegant, but it kept them compliant.\nWhat surprised people most was how regulators interpreted the idea of ‚Äúhigh-risk systems.‚Äù You don‚Äôt need to train a massive model to qualify. In some cases, building a scoring sheet in Excel that influences hiring, credit checks, or insurance pricing was enough to trigger additional documentation. That pushed analysts working with basic business intelligence (BI) tools into the same regulatory bucket as machine learning engineers.\n#\n2. Spain‚Äôs 2025 Crackdown: Up to ‚Ç¨35 M Fines for Unlabeled AI Content\nIn March 2025, Spain took a bold step:\nits government approved a draft law that would fine companies as much as ‚Ç¨35 million or 7% of their global turnover\nif they fail to clearly label AI-generated content. The move aimed at cracking down on ‚Äúdeepfakes‚Äù and misleading media, but its reach goes far beyond flashy images or viral videos. For anyone working with data, this law shifts the ground under how you process, present, and publish AI-assisted content.\nUnder the proposed regulation, any content generated or manipulated by artificial intelligence (images, video, audio, or text) must be\nclearly labeled as AI-generated\n. Failing to do so counts as a ‚Äú\nserious offense\n.‚Äù\nThe law doesn‚Äôt only target deepfakes. It also bans manipulative uses of AI that exploit vulnerable people, such as subliminal messaging or AI-powered profiling based on sensitive attributes (biometrics, social media behavior, etc.).\nYou might ask, why should analysts care? At first glance, this might seem like a law for social media companies, media houses, or big tech companies. But it quickly affects everyday data and analytics workflows in three broad ways:\n1. AI-generated tables, summaries, and charts need labeling\n: Analysts are increasingly using generative AI tools to create parts of reports, such as summaries, visualizations, annotated charts, and tables derived from data transformations. Under Spain‚Äôs law, any output created or substantially modified by AI must be labeled as such before dissemination. That means your internal dashboards, BI reports, slide decks, and anything shared beyond your machine may require visible AI content disclosure.\n2. Published findings must carry provenance metadata\n: If your report combines human-processed data with AI-generated insights (e.g. a model-generated forecast, a cleaned dataset, automatically generated documentation), you now have a compliance requirement. Forgetting to label a chart or an AI-generated paragraph could result in a heavy fine.\n3. Data-handling pipelines and audits matter more than ever\n: Because the new law doesn‚Äôt only cover public content, but also tools and internal systems, analysts working in Python, R, Excel, or any data-processing environment must be mindful about which parts of pipelines involve AI. Teams may need to build internal documentation, track usage of AI modules, log which dataset transformations used AI, and version control every step, all to ensure transparency if regulators audit.\nLet's look at the risks. The numbers are serious: the proposed bill sets fines between\n‚Ç¨7.5 million and ‚Ç¨35 million, or 2‚Äì7% of a company‚Äôs global revenue\n, depending on size and severity of violation. For large firms operating across borders, the ‚Äúglobal turnover‚Äù clause means many will choose to over-comply rather than risk non-compliance.\nGiven this new reality, here‚Äôs what analysts working today should consider:\nAudit your workflows to identify where AI tools (large language models, image generators, and auto-cleanup scripts) interact with your data or content.\nAdd provenance metadata for any AI-assisted output, mark it clearly (‚ÄúGenerated with AI / Reviewed by analyst / Date‚Äù)\nPerform version control, document pipelines, and ensure that each transformation step (especially AI-driven ones) is traceable\nEducate your team so they are aware that transparency and compliance are part of their data-handling culture, not an afterthought\n#\n3. The U.S. Privacy Patchwork Expanded in 2025\nIn 2025, a wave of U.S. states updated or introduced comprehensive data-privacy laws. For analysts working on any data stack that touches personal data, this means stricter expectations for data collection, storage, and profiling.\nWhat Changed? Several states activated new privacy laws in 2025. For example:\nThe Nebraska Data Privacy Act\n, Delaware Personal Data Privacy Act, and New Hampshire Consumer Data Privacy Act all took effect January 1, 2025\nThe Maryland Online Data Privacy Act (MODPA)\nbecame effective on October 1, 2025, one of the strictest laws passed this year\nThese laws share broad themes: they compel companies to limit data collection to what‚Äôs strictly necessary, require transparency and rights for data subjects (including access, deletion, and opt-out), and impose new restrictions on how ‚Äú\nsensitive\n‚Äù data (such as health, biometric, or profiling data) may be processed.\nFor teams inside the U.S. handling user data, customer records, or analytics datasets, the impact is real. These laws affect how data pipelines are designed, how storage and exports are handled, and what kind of profiling or segmentation you may run.\nIf you work with data, here‚Äôs what the new landscape demands:\nYou must justify the collection, which means that every field in a dataset aimed for storage or every column in a CSV needs a documented purpose. Collecting more ‚Äú\njust in case\n‚Äù data may no longer be defensible under these laws.\nSensitive data requires tracking and clearance. Therefore, if a field contains or implies sensitive data, it may require explicit consent and stronger protection, or be excluded altogether.\nIf you run segmentation, scoring, or profiling (e.g. credit scoring, recommendation, targeting), check whether your state‚Äôs law treats that as ‚Äú\nsensitive\n‚Äù or ‚Äú\nspecial-category\n‚Äù data and whether your processing qualifies under the law.\nThese laws often include rights to deletion or correction. That means your data exports, database snapshots, or logs need processes for removal or anonymization.\nBefore 2025, many U.S. teams operated under loose assumptions: collect what might be useful, store raw dumps, analyze freely, and anonymize later if needed. That approach is becoming risky. The new laws don‚Äôt target specific tools, languages, or frameworks; they target data practices. That means whether you use R, Python, SQL, Excel, or a BI tool, you all face the same rules.\n#\n4. Shadow AI Became a Compliance Hazard, Even Without a Breach\nIn 2025, regulators and security teams began to view unsanctioned AI use as more than just a productivity issue. \"Shadow AI\" ‚Äî employees using public large language models (LLMs) and other AI tools without IT approval ‚Äî moved from just being a compliance footnote to a board-level risk. Often, it looked like auditors found evidence that staff pasted customer records into a public chat service, or internal investigations that showed sensitive data flowing into unmonitored AI tools. Those findings led to internal discipline, regulatory scrutiny, and, in several sectors, formal inquiries.\nThe technical and regulatory response hardened quickly. Industry bodies and security vendors have warned that shadow AI creates a new, invisible attack surface, as models ingest corporate secrets, training data, or personal information that then leaves any corporate control or audit trail.\nThe National Institute of Standards and Technology (NIST)\nand security vendors published guidance and best practices aimed at discovery and containment on how to detect unauthorized AI use, set up approved AI gateways, and apply redaction or data loss prevention (DLP) before anything goes to a third-party model. For regulated sectors, auditors began to expect proof that employees cannot simply paste raw records into consumer AI services.\nFor analysts, here are the implications: teams no longer rely on the ‚Äú\nquick query in ChatGPT\n‚Äù habit for exploratory work. Organizations required explicit, logged approvals for any dataset sent to an external AI service.\nWhere do we go from here?\nStop pasting PII into consumer LLMs\nUse an approved enterprise AI gateway or on-prem model for exploratory work\nAdd a pre-send redaction step to scripts and notebooks, and insist your team archives prompts and outputs for auditability\n#\n5. Data Lineage Enforcement Went Mainstream\nThis year, regulators, auditors, and major companies have increasingly demanded that every dataset, transformation, and output can be traced from source to end product. What used to be a ‚Äú\nnice to have\n‚Äù for large data teams is quickly becoming a compliance requirement.\nA major trigger came from corporate compliance teams themselves. Several large firms, particularly those operating across multiple regions, have begun tightening their internal audit requirements. They need to show, not just tell, where data originates and how it flows through pipelines before it ends up in reports, dashboards, models, or exports.\nOne public example:\nMeta\npublished details of an internal data-lineage system that tracks data flows at scale. Their ‚ÄúPolicy Zone Manager‚Äù tool automatically tags and traces data from ingestion through processing to final storage or use. This move is part of a broader push to embed privacy and provenance into engineering practices.\nIf you work with data in Python, R, SQL, Excel, or any analytics stack, the demands now go beyond correctness or format. The questions become: Where did the data come from? Which scripts or transformations touched it? Which version of the dataset fed a particular chart or report?\nThis affects everyday tasks:\nWhen exporting a cleaned CSV, you must tag it with source, cleaning date, and transformation history\nWhen running an analytics script, you need version control, documentation of inputs, and provenance metadata\nFeeding data into model or dashboard systems, or manual logs, must record exactly which rows/columns, when, and from where\nIf you don‚Äôt already track lineage and provenance, 2025 makes it urgent. Here‚Äôs a practical starting checklist:\nFor every data import or ingestion; store metadata (source, date, user, version)\nFor each transformation or cleaning step, commit the changes (in version control or logs) along with a brief description\nFor exports, reports, and dashboards, include provenance metadata, such as dataset version, transformation script version, and timestamp\nFor analytic models or dashboards fed by data: attach lineage tags so viewers and auditors know exactly what feed, when, and from where\nPrefer tools or frameworks that support lineage or provenance (e.g. internal tooling, built-in data lineage tracking, or external libraries)\n#\nConclusion\nFor analysts, these stories are not abstract; they are real. They shape your day-to-day work. The EU AI Act‚Äôs phased rollout has changed how you document model workflows. Spain‚Äôs aggressive stance on unlabeled AI has raised the bar for transparency in even simple analytics dashboards. The U.S. push to merge AI governance with privacy rules forces teams to revisit their data flows and risk documentation.\nIf you take anything from these five stories, let it be this: data privacy is no longer something handed off to legal or compliance. It‚Äôs embedded in the work analysts do every day. Version your inputs. Label your data. Trace your transformations. Document your models. Keep track of why your dataset exists in the first place. These habits now serve as your professional safety net.\nShittu Olumide\nis a software engineer and technical writer passionate about leveraging cutting-edge technologies to craft compelling narratives, with a keen eye for detail and a knack for simplifying complex concepts. You can also find Shittu on\nTwitter\n.\n<= Previous post\nNext post =>",
    "full_html": "<div class=\"single\" id=\"content\">\n\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<p><center><img alt=\"5 Data Privacy Stories from 2025 Every Analyst Should Know\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/kdn-olumide-5-data-privacy-stories-from-2025-analysts-should-know.png\" width=\"100%\"><br>\n<font size=\"-1\">Image by Editor</font></br></img></center><br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Introduction</h2>\n<p>¬†<br/>\nIf you work with data for a living, 2025 has probably felt different. Privacy used to be something your legal team handled in a long PDF nobody read. This year, it crept straight into everyday analytics work. The rules changed, and suddenly, people who write R scripts, clean CSVs in Python, build Excel dashboards, or ship weekly reports are expected to understand how their choices affect compliance.</p>\n<p>That shift didn‚Äôt happen because regulators started caring more about data. It happened because data analysis is where privacy problems actually show up. A single unlabeled AI-generated chart, an extra column left in a dataset, or a model trained on undocumented data can put a company on the wrong side of the law. And in 2025, regulators stopped giving warnings and started handing out real penalties.</p><div class=\"kdnug-3ae68b6c885207f659d0f479e9a4b467 kdnug-in-content-1\" id=\"kdnug-3ae68b6c885207f659d0f479e9a4b467\" style=\"padding-bottom:180px; padding-top:20px\"></div>\n<p>In this article, we will take a look at five specific stories from 2025 that should matter to anyone who touches data. These aren‚Äôt abstract trends or high-level policy notes. They‚Äôre real events that changed how analysts work day to day, from the code you write to the reports you publish.</p>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>1. The EU AI Act‚Äôs First Enforcement Phase Hit Analysts Harder Than Developers</h2>\n<p>¬†<br/>\nWhen the <strong><a href=\"https://artificialintelligenceact.eu/\" target=\"_blank\">EU AI Act</a></strong> officially moved into its first enforcement phase in early 2025, most teams expected model builders and machine learning leads to feel the pressure. Instead, the first wave of compliance work landed squarely on analysts. The reason was simple: regulators focused on data inputs and documentation, not just AI model behavior.</p>\n<p>Across Europe, companies were suddenly required to prove where training data came from, how it was labeled, and whether any AI-generated content inside their datasets was clearly marked. That meant analysts had to rebuild the very basics of their workflow. R notebooks needed provenance notes. Python pipelines needed metadata fields for ‚Äú<strong>synthetic</strong> vs. <strong>real</strong>.‚Äù Even shared Excel workbooks had to carry small disclaimers explaining whether AI was used to clean or transform the data.</p>\n<p>Teams also learned quickly that ‚ÄúAI transparency‚Äù is not a developer-only concept. If an analyst used <strong><a href=\"https://github.com/features/copilot\" target=\"_blank\">Copilot</a></strong>, <strong><a href=\"https://gemini.google.com/\" target=\"_blank\">Gemini</a></strong>, or <strong><a href=\"https://openai.com/chatgpt\" target=\"_blank\">ChatGPT</a></strong> to write part of a query or generate a quick summary table, the output needed to be identified as AI-assisted in regulated industries. For many teams, that meant adopting a simple tagging practice, something as basic as adding a short metadata note like ‚Äú<em>Generated with AI, validated by analyst</em>.‚Äù It wasn‚Äôt elegant, but it kept them compliant.</p>\n<p>What surprised people most was how regulators interpreted the idea of ‚Äúhigh-risk systems.‚Äù You don‚Äôt need to train a massive model to qualify. In some cases, building a scoring sheet in Excel that influences hiring, credit checks, or insurance pricing was enough to trigger additional documentation. That pushed analysts working with basic business intelligence (BI) tools into the same regulatory bucket as machine learning engineers.</p>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>2. Spain‚Äôs 2025 Crackdown: Up to ‚Ç¨35 M Fines for Unlabeled AI Content</h2>\n<p>¬†<br/>\nIn March 2025, Spain took a bold step: <strong><a href=\"https://www.euronews.com/next/2025/03/12/spain-could-fine-ai-companies-up-to-35-million-in-fines-for-mislabelling-content\" target=\"_blank\">its government approved a draft law that would fine companies as much as ‚Ç¨35 million or 7% of their global turnover</a></strong> if they fail to clearly label AI-generated content. The move aimed at cracking down on ‚Äúdeepfakes‚Äù and misleading media, but its reach goes far beyond flashy images or viral videos. For anyone working with data, this law shifts the ground under how you process, present, and publish AI-assisted content.</p>\n<p>Under the proposed regulation, any content generated or manipulated by artificial intelligence (images, video, audio, or text) must be <strong><a href=\"https://www.next-step.es/en/spain-approves-new-ia-governance-law-million-dollar-fines-for-failing-to-label-content/\" target=\"_blank\">clearly labeled as AI-generated</a></strong>. Failing to do so counts as a ‚Äú<strong>serious offense</strong>.‚Äù</p>\n<p>The law doesn‚Äôt only target deepfakes. It also bans manipulative uses of AI that exploit vulnerable people, such as subliminal messaging or AI-powered profiling based on sensitive attributes (biometrics, social media behavior, etc.).</p>\n<p>You might ask, why should analysts care? At first glance, this might seem like a law for social media companies, media houses, or big tech companies. But it quickly affects everyday data and analytics workflows in three broad ways:</p>\n<ol>\n<li><strong>1. AI-generated tables, summaries, and charts need labeling</strong>: Analysts are increasingly using generative AI tools to create parts of reports, such as summaries, visualizations, annotated charts, and tables derived from data transformations. Under Spain‚Äôs law, any output created or substantially modified by AI must be labeled as such before dissemination. That means your internal dashboards, BI reports, slide decks, and anything shared beyond your machine may require visible AI content disclosure.\n<li><strong>2. Published findings must carry provenance metadata</strong>: If your report combines human-processed data with AI-generated insights (e.g. a model-generated forecast, a cleaned dataset, automatically generated documentation), you now have a compliance requirement. Forgetting to label a chart or an AI-generated paragraph could result in a heavy fine.\n<li><strong>3. Data-handling pipelines and audits matter more than ever</strong>: Because the new law doesn‚Äôt only cover public content, but also tools and internal systems, analysts working in Python, R, Excel, or any data-processing environment must be mindful about which parts of pipelines involve AI. Teams may need to build internal documentation, track usage of AI modules, log which dataset transformations used AI, and version control every step, all to ensure transparency if regulators audit.\n</li></li></li></ol>\n<p>Let's look at the risks. The numbers are serious: the proposed bill sets fines between <strong>‚Ç¨7.5 million and ‚Ç¨35 million, or 2‚Äì7% of a company‚Äôs global revenue</strong>, depending on size and severity of violation. For large firms operating across borders, the ‚Äúglobal turnover‚Äù clause means many will choose to over-comply rather than risk non-compliance.</p>\n<p>Given this new reality, here‚Äôs what analysts working today should consider:</p>\n<ul>\n<li>Audit your workflows to identify where AI tools (large language models, image generators, and auto-cleanup scripts) interact with your data or content.\n<li>Add provenance metadata for any AI-assisted output, mark it clearly (‚ÄúGenerated with AI / Reviewed by analyst / Date‚Äù)\n<li>Perform version control, document pipelines, and ensure that each transformation step (especially AI-driven ones) is traceable\n<li>Educate your team so they are aware that transparency and compliance are part of their data-handling culture, not an afterthought\n</li></li></li></li></ul>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>3. The U.S. Privacy Patchwork Expanded in 2025</h2>\n<p>¬†<br/>\nIn 2025, a wave of U.S. states updated or introduced comprehensive data-privacy laws. For analysts working on any data stack that touches personal data, this means stricter expectations for data collection, storage, and profiling.</p>\n<p>What Changed? Several states activated new privacy laws in 2025. For example:</p>\n<ul>\n<li><strong><a href=\"https://www.whitecase.com/insight-alert/data-privacy-update-2025\" target=\"_blank\">The Nebraska Data Privacy Act</a></strong>, Delaware Personal Data Privacy Act, and New Hampshire Consumer Data Privacy Act all took effect January 1, 2025\n<li><strong><a href=\"https://www.afslaw.com/perspectives/privacy-counsel/new-state-privacy-laws-second-half-2025\" target=\"_blank\">The Maryland Online Data Privacy Act (MODPA)</a></strong> became effective on October 1, 2025, one of the strictest laws passed this year\n</li></li></ul>\n<p>These laws share broad themes: they compel companies to limit data collection to what‚Äôs strictly necessary, require transparency and rights for data subjects (including access, deletion, and opt-out), and impose new restrictions on how ‚Äú<strong>sensitive</strong>‚Äù data (such as health, biometric, or profiling data) may be processed.</p>\n<p>For teams inside the U.S. handling user data, customer records, or analytics datasets, the impact is real. These laws affect how data pipelines are designed, how storage and exports are handled, and what kind of profiling or segmentation you may run.</p>\n<p>If you work with data, here‚Äôs what the new landscape demands:</p>\n<ul>\n<li>You must justify the collection, which means that every field in a dataset aimed for storage or every column in a CSV needs a documented purpose. Collecting more ‚Äú<em>just in case</em>‚Äù data may no longer be defensible under these laws.\n<li>Sensitive data requires tracking and clearance. Therefore, if a field contains or implies sensitive data, it may require explicit consent and stronger protection, or be excluded altogether.\n<li>If you run segmentation, scoring, or profiling (e.g. credit scoring, recommendation, targeting), check whether your state‚Äôs law treats that as ‚Äú<strong>sensitive</strong>‚Äù or ‚Äú<strong>special-category</strong>‚Äù data and whether your processing qualifies under the law.\n<li>These laws often include rights to deletion or correction. That means your data exports, database snapshots, or logs need processes for removal or anonymization.\n</li></li></li></li></ul>\n<p>Before 2025, many U.S. teams operated under loose assumptions: collect what might be useful, store raw dumps, analyze freely, and anonymize later if needed. That approach is becoming risky. The new laws don‚Äôt target specific tools, languages, or frameworks; they target data practices. That means whether you use R, Python, SQL, Excel, or a BI tool, you all face the same rules.</p>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>4. Shadow AI Became a Compliance Hazard, Even Without a Breach</h2>\n<p>¬†<br/>\nIn 2025, regulators and security teams began to view unsanctioned AI use as more than just a productivity issue. \"Shadow AI\" ‚Äî employees using public large language models (LLMs) and other AI tools without IT approval ‚Äî moved from just being a compliance footnote to a board-level risk. Often, it looked like auditors found evidence that staff pasted customer records into a public chat service, or internal investigations that showed sensitive data flowing into unmonitored AI tools. Those findings led to internal discipline, regulatory scrutiny, and, in several sectors, formal inquiries.</p>\n<p>The technical and regulatory response hardened quickly. Industry bodies and security vendors have warned that shadow AI creates a new, invisible attack surface, as models ingest corporate secrets, training data, or personal information that then leaves any corporate control or audit trail. <strong><a href=\"https://www.nist.gov/itl/ai-risk-management-framework\" target=\"_blank\">The National Institute of Standards and Technology (NIST)</a></strong> and security vendors published guidance and best practices aimed at discovery and containment on how to detect unauthorized AI use, set up approved AI gateways, and apply redaction or data loss prevention (DLP) before anything goes to a third-party model. For regulated sectors, auditors began to expect proof that employees cannot simply paste raw records into consumer AI services.</p>\n<p>For analysts, here are the implications: teams no longer rely on the ‚Äú<em>quick query in ChatGPT</em>‚Äù habit for exploratory work. Organizations required explicit, logged approvals for any dataset sent to an external AI service.</p>\n<p>Where do we go from here?</p>\n<ul>\n<li>Stop pasting PII into consumer LLMs\n<li>Use an approved enterprise AI gateway or on-prem model for exploratory work\n<li>Add a pre-send redaction step to scripts and notebooks, and insist your team archives prompts and outputs for auditability\n</li></li></li></ul>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>5. Data Lineage Enforcement Went Mainstream</h2>\n<p>¬†<br/>\nThis year, regulators, auditors, and major companies have increasingly demanded that every dataset, transformation, and output can be traced from source to end product. What used to be a ‚Äú<em>nice to have</em>‚Äù for large data teams is quickly becoming a compliance requirement.</p>\n<p>A major trigger came from corporate compliance teams themselves. Several large firms, particularly those operating across multiple regions, have begun tightening their internal audit requirements. They need to show, not just tell, where data originates and how it flows through pipelines before it ends up in reports, dashboards, models, or exports.</p>\n<p>One public example: <strong><a href=\"https://engineering.fb.com/2025/01/22/security/how-meta-discovers-data-flows-via-lineage-at-scale/\" target=\"_blank\">Meta</a></strong> published details of an internal data-lineage system that tracks data flows at scale. Their ‚ÄúPolicy Zone Manager‚Äù tool automatically tags and traces data from ingestion through processing to final storage or use. This move is part of a broader push to embed privacy and provenance into engineering practices.</p>\n<p>If you work with data in Python, R, SQL, Excel, or any analytics stack, the demands now go beyond correctness or format. The questions become: Where did the data come from? Which scripts or transformations touched it? Which version of the dataset fed a particular chart or report?</p>\n<p>This affects everyday tasks:</p>\n<ul>\n<li>When exporting a cleaned CSV, you must tag it with source, cleaning date, and transformation history\n<li>When running an analytics script, you need version control, documentation of inputs, and provenance metadata\n<li>Feeding data into model or dashboard systems, or manual logs, must record exactly which rows/columns, when, and from where\n</li></li></li></ul>\n<p>If you don‚Äôt already track lineage and provenance, 2025 makes it urgent. Here‚Äôs a practical starting checklist:</p>\n<ol>\n<li>For every data import or ingestion; store metadata (source, date, user, version)\n<li>For each transformation or cleaning step, commit the changes (in version control or logs) along with a brief description\n<li>For exports, reports, and dashboards, include provenance metadata, such as dataset version, transformation script version, and timestamp\n<li>For analytic models or dashboards fed by data: attach lineage tags so viewers and auditors know exactly what feed, when, and from where\n<li>Prefer tools or frameworks that support lineage or provenance (e.g. internal tooling, built-in data lineage tracking, or external libraries)\n</li></li></li></li></li></ol>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Conclusion</h2>\n<p>¬†<br/>\nFor analysts, these stories are not abstract; they are real. They shape your day-to-day work. The EU AI Act‚Äôs phased rollout has changed how you document model workflows. Spain‚Äôs aggressive stance on unlabeled AI has raised the bar for transparency in even simple analytics dashboards. The U.S. push to merge AI governance with privacy rules forces teams to revisit their data flows and risk documentation.</p>\n<p>If you take anything from these five stories, let it be this: data privacy is no longer something handed off to legal or compliance. It‚Äôs embedded in the work analysts do every day. Version your inputs. Label your data. Trace your transformations. Document your models. Keep track of why your dataset exists in the first place. These habits now serve as your professional safety net.<br/>\n¬†<br/>\n¬†</p>\n<p><a href=\"https://www.linkedin.com/in/olumide-shittu\"><strong><strong><a href=\"https://www.linkedin.com/in/olumide-shittu/\" rel=\"noopener noreferrer\" target=\"_blank\">Shittu Olumide</a></strong></strong></a> is a software engineer and technical writer passionate about leveraging cutting-edge technologies to craft compelling narratives, with a keen eye for detail and a knack for simplifying complex concepts. You can also find Shittu on <a href=\"https://twitter.com/Shittu_Olumide_\">Twitter</a>.</p>\n</div><br/>\n<!--<hr class=\"grey-line\"><br>\t\n  <div><h3>Our Top 5 Free Course Recommendations</h3><br>\n\t  \t</div>\t\n-->\n<div class=\"box-form\">\n<!-- Mailchimp for WordPress v4.10.9 - https://wordpress.org/plugins/mailchimp-for-wp/ --><!-- / Mailchimp for WordPress Plugin --></div>\n<!-- You can start editing here. -->\n\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<br/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/5-workflow-automation-tools-for-all-professionals\" rel=\"prev\">&lt;= Previous post</a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/how-to-handle-large-datasets-in-python-even-if-youre-a-beginner\" rel=\"next\">Next post =&gt;</a></div>\n<br/><br/>\n</div>\n<!--#content end--></div>",
    "thumbnail": "https://www.kdnuggets.com/wp-content/uploads/kdn-olumide-5-data-privacy-stories-from-2025-analysts-should-know.png",
    "word_count": 2239,
    "scraped_at": "2025-12-17T18:52:50.515011"
  },
  {
    "title": "5 Workflow Automation Tools for All Professionals - KDnuggets",
    "url": "https://www.kdnuggets.com/5-workflow-automation-tools-for-all-professionals",
    "summary": "Five powerful automation tools to help you streamline repetitive digital tasks, increase productivity, and create smarter workflows without requiring deep technical skills.",
    "image_url": "https://www.kdnuggets.com/wp-content/uploads/awan_5_workflow_automation_tools_professionals_6.png",
    "published_date": null,
    "author": "https://www.facebook.com/kdnuggets",
    "full_text": "Image by Author\n#\nIntroduction\nAutomation can benefit professionals across various fields, including project managers, analysts, and solo founders. We all face repetitive digital tasks that consume our time, such as gathering data from the web, cleaning and standardizing it, updating spreadsheets, and creating clear, actionable reports.\nIn this article, we will explore five powerful yet user-friendly workflow automation tools. With features like drag-and-drop nodes, prebuilt connectors, and guided templates, you can easily create end-to-end workflows without the need for extensive engineering knowledge.\n#\n1. n8n\nn8n\nis an open-source, AI-native workflow automation platform that empowers users to build complex automations with a visual, node-based editor. It‚Äôs especially popular among technical professionals and teams who value flexibility, extensibility, and data privacy.\nKey Features:\nVisual Workflow Builder:\nDrag-and-drop interface with branching, looping, and error handling.\nAI Integration:\nNative support for LangChain, LLMs (OpenAI, Gemini, Claude), and vector databases for advanced AI workflows.\nCustom Code:\nInject JavaScript or Python for custom logic; develop your own nodes.\nDeployment:\nFree, unlimited self-hosting or managed cloud options.\nCollaboration:\nVersion control, access management, and workflow sharing.\n#\n2. Zapier\nZapier\nis the market leader in no-code workflow automation, connecting over 8,000 apps. Its intuitive interface and vast template library make it accessible for non-technical users and small businesses, while its AI orchestration features appeal to innovators and enterprises.\nKey Features:\nApp Ecosystem:\nLargest integration catalog (8,000+ apps, 500+ AI tools)\nNo-Code Builder:\nDrag-and-drop ‚ÄúZap‚Äù creation with multi-step workflows\nAI Orchestration:\nSecurely connect AI agents (OpenAI, Gemini) to business apps\nTemplates:\nThousands of pre-built workflows for rapid deployment\nEnterprise Features:\nAdmin controls, advanced security, and team collaboration\n#\n3. Make\nMake\noffers a powerful, visual-first automation platform that balances ease of use with advanced capabilities. Its scenario-based builder supports complex, multi-app workflows with conditional logic, making it ideal for both beginners and power users.\nKey Features:\nVisual Scenario Builder:\nDrag-and-drop modules, branching, filtering, and error handling\nAdvanced Automation:\nMulti-step workflows, real-time/scheduled triggers, API integration\nAI-Powered:\nNatural language workflow creation, AI agents, 400+ AI app integrations\nIntegration Catalog:\n2,000+ SaaS tools, 3,000+ pre-built connectors\nEnterprise Solutions:\nVisual orchestration, enhanced security, team collaboration\n#\n4. Microsoft Power Automate\nPower Automate\nis Microsoft‚Äôs flagship automation tool, tightly integrated with the Microsoft 365 ecosystem. It offers both cloud and desktop automation (RPA), making it a top choice for organizations already invested in Microsoft technologies.\nKey Features:\nMicrosoft Integration:\nAutomate across Teams, SharePoint, OneDrive, Dynamics 365, and Azure\nRPA Capabilities:\nAttended/unattended desktop automation, hosted RPA on Azure VMs\nAI Builder:\nLow-code AI for document processing, sentiment analysis, and more\nTemplates & Connectors:\n1,000+ connectors, vast template library\nEnterprise Security:\nRole-based access, compliance, audit logging\n#\n5. ClickUp\nClickUp\nis an all-in-one project management platform with built-in workflow automation. It stands out for blending task management, documentation, collaboration, and AI-driven automation in a single workspace.\nKey Features:\nNative Automations:\nConditional logic, multi-step workflows, and workspace-level triggers\nAI-Powered:\nClickUp Brain for natural language automation and AI agents\nTemplates:\nWorkflow templates for common business processes\nIntegrations:\n1,000+ native integrations, plus API and third-party automation support\nCollaboration:\nReal-time chat, comments, and Docs integrated with automations\n#\nSummary\nThe table below provides a quick overview of the top automation tools, summarizing who they are best for, their AI capabilities, integrations, pricing models, self-hosting options, and learning curve.\nTool\nBest For\nAI Features\nIntegrations\nPricing Model\nSelf-Hosting\nLearning Curve\nn8n\nTechnical teams, AI workflows\nDeep, native\n400+\nFree/cloud/enterprise\nYes\nModerate-High\nZapier\nNon-technical, SMBs, enterprise\nStrong, orchestration\n8,000+\nFree/task-based\nNo\nVery Low\nMake\nMixed teams, complex workflows\nAdvanced, visual\n2,000+\nFree/operation-based\nNo\nModerate\nPower Automate\nMicrosoft-centric enterprises\nAI Builder, RPA\n1,000+\nUser/bot/pay-as-you-go\nNo\nModerate\nClickUp\nTeams, project management\nAI Brain, agents\n1,000+\nUser-based/add-ons\nNo\nLow (AI/no-code)\nAbid Ali Awan\n(\n@1abidaliawan\n) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in technology management and a bachelor's degree in telecommunication engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness.\n<= Previous post\nNext post =>",
    "full_html": "<div class=\"single\" id=\"content\">\n\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<p><center><img alt=\"5 Workflow Automation Tools for All Professionals\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/awan_5_workflow_automation_tools_professionals_6.png\" width=\"100%\"><br>\n<font size=\"-1\">Image by Author</font></br></img></center><br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Introduction</h2>\n<p>¬†<br/>\nAutomation can benefit professionals across various fields, including project managers, analysts, and solo founders. We all face repetitive digital tasks that consume our time, such as gathering data from the web, cleaning and standardizing it, updating spreadsheets, and creating clear, actionable reports.¬†</p><div class=\"kdnug-8853367366b9421bd15f8cb3247ab5b1 kdnug-after-first-paragraph\" id=\"kdnug-8853367366b9421bd15f8cb3247ab5b1\"></div>\n<p>In this article, we will explore five powerful yet user-friendly workflow automation tools. With features like drag-and-drop nodes, prebuilt connectors, and guided templates, you can easily create end-to-end workflows without the need for extensive engineering knowledge.¬†</p><div class=\"kdnug-e4187cf25ba8b7597f0ea98f7c44fde6 kdnug-in-content-1\" id=\"kdnug-e4187cf25ba8b7597f0ea98f7c44fde6\" style=\"padding-bottom:180px; padding-top:20px\"></div>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>1. n8n</h2>\n<p>¬†<br/>\n<a href=\"https://n8n.io/\" rel=\"noopener\" target=\"_blank\">n8n</a> is an open-source, AI-native workflow automation platform that empowers users to build complex automations with a visual, node-based editor. It‚Äôs especially popular among technical professionals and teams who value flexibility, extensibility, and data privacy.</p>\n<p>¬†<br/>\n<center><img alt=\"5 Workflow Automation Tools for All Professionals\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/awan_5_workflow_automation_tools_professionals_5.png\" width=\"100%\"/></center><br/>\n¬† </p>\n<p><strong>Key Features:</strong></p>\n<ul>\n<li><strong>Visual Workflow Builder: </strong>Drag-and-drop interface with branching, looping, and error handling.\n<li><strong>AI Integration: </strong>Native support for LangChain, LLMs (OpenAI, Gemini, Claude), and vector databases for advanced AI workflows.\n<li><strong>Custom Code:</strong> Inject JavaScript or Python for custom logic; develop your own nodes.\n<li><strong>Deployment:</strong> Free, unlimited self-hosting or managed cloud options.\n<li><strong>Collaboration: </strong>Version control, access management, and workflow sharing.\n</li></li></li></li></li></ul>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>2. Zapier</h2>\n<p>¬†<br/>\n<a href=\"https://zapier.com/\" rel=\"noopener\" target=\"_blank\">Zapier</a> is the market leader in no-code workflow automation, connecting over 8,000 apps. Its intuitive interface and vast template library make it accessible for non-technical users and small businesses, while its AI orchestration features appeal to innovators and enterprises.</p>\n<p>¬†<br/>\n<center><img alt=\"5 Workflow Automation Tools for All Professionals\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/awan_5_workflow_automation_tools_professionals_3.png\" width=\"100%\"/></center><br/>\n¬† </p>\n<p><strong>Key Features:</strong></p>\n<ul>\n<li><strong>App Ecosystem:</strong> Largest integration catalog (8,000+ apps, 500+ AI tools)\n<li><strong>No-Code Builder: </strong>Drag-and-drop ‚ÄúZap‚Äù creation with multi-step workflows\n<li><strong>AI Orchestration:</strong> Securely connect AI agents (OpenAI, Gemini) to business apps\n<li><strong>Templates:</strong> Thousands of pre-built workflows for rapid deployment\n<li><strong>Enterprise Features: </strong>Admin controls, advanced security, and team collaboration\n</li></li></li></li></li></ul>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>3. Make</h2>\n<p>¬†<br/>\n<a href=\"https://www.make.com/en\" rel=\"noopener\" target=\"_blank\">Make</a> offers a powerful, visual-first automation platform that balances ease of use with advanced capabilities. Its scenario-based builder supports complex, multi-app workflows with conditional logic, making it ideal for both beginners and power users.</p>\n<p>¬†<br/>\n<center><img alt=\"5 Workflow Automation Tools for All Professionals\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/awan_5_workflow_automation_tools_professionals_4.png\" width=\"100%\"/></center><br/>\n¬† </p>\n<p><strong>Key Features:</strong></p>\n<ul>\n<li><strong>Visual Scenario Builder:</strong> Drag-and-drop modules, branching, filtering, and error handling\n<li><strong>Advanced Automation:</strong> Multi-step workflows, real-time/scheduled triggers, API integration\n<li><strong>AI-Powered: </strong>Natural language workflow creation, AI agents, 400+ AI app integrations\n<li><strong>Integration Catalog:</strong> 2,000+ SaaS tools, 3,000+ pre-built connectors\n<li><strong>Enterprise Solutions:</strong> Visual orchestration, enhanced security, team collaboration\n</li></li></li></li></li></ul>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>4. Microsoft Power Automate</h2>\n<p>¬†<br/>\n<a href=\"https://www.microsoft.com/en-us/power-platform/products/power-automate/\" rel=\"noopener\" target=\"_blank\">Power Automate</a> is Microsoft‚Äôs flagship automation tool, tightly integrated with the Microsoft 365 ecosystem. It offers both cloud and desktop automation (RPA), making it a top choice for organizations already invested in Microsoft technologies.</p>\n<p>¬†<br/>\n<center><img alt=\"5 Workflow Automation Tools for All Professionals\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/awan_5_workflow_automation_tools_professionals_2.png\" width=\"100%\"/></center><br/>\n¬† </p>\n<p><strong>Key Features:</strong></p>\n<ul>\n<li><strong>Microsoft Integration:</strong> Automate across Teams, SharePoint, OneDrive, Dynamics 365, and Azure\n<li><strong>RPA Capabilities: </strong>Attended/unattended desktop automation, hosted RPA on Azure VMs\n<li><strong>AI Builder:</strong> Low-code AI for document processing, sentiment analysis, and more\n<li><strong>Templates &amp; Connectors:</strong> 1,000+ connectors, vast template library\n<li><strong>Enterprise Security: </strong>Role-based access, compliance, audit logging\n</li></li></li></li></li></ul>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>5. ClickUp</h2>\n<p>¬†<br/>\n<a href=\"https://clickup.com/\" rel=\"noopener\" target=\"_blank\">ClickUp</a> is an all-in-one project management platform with built-in workflow automation. It stands out for blending task management, documentation, collaboration, and AI-driven automation in a single workspace.</p>\n<p>¬†<br/>\n<center><img alt=\"5 Workflow Automation Tools for All Professionals\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/awan_5_workflow_automation_tools_professionals_1.png\" width=\"100%\"/></center><br/>\n¬† </p>\n<p><strong>Key Features:</strong></p>\n<ul>\n<li><strong>Native Automations:</strong> Conditional logic, multi-step workflows, and workspace-level triggers\n<li><strong>AI-Powered:</strong> ClickUp Brain for natural language automation and AI agents\n<li><strong>Templates:</strong> Workflow templates for common business processes\n<li><strong>Integrations: </strong>1,000+ native integrations, plus API and third-party automation support\n<li><strong>Collaboration:</strong> Real-time chat, comments, and Docs integrated with automations\n</li></li></li></li></li></ul>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Summary</h2>\n<p>¬†<br/>\nThe table below provides a quick overview of the top automation tools, summarizing who they are best for, their AI capabilities, integrations, pricing models, self-hosting options, and learning curve.</p>\n<p>¬†</p>\n<table style=\"width: 100%; border-collapse: collapse; font-family: Arial, sans-serif; font-size: 14px\">\n<thead>\n<tr>\n<th style=\"padding: 12px; border: 1px solid #ddd; text-align: left\">Tool</th>\n<th style=\"padding: 12px; border: 1px solid #ddd; text-align: left\">Best For</th>\n<th style=\"padding: 12px; border: 1px solid #ddd; text-align: left\">AI Features</th>\n<th style=\"padding: 12px; border: 1px solid #ddd; text-align: left\">Integrations</th>\n<th style=\"padding: 12px; border: 1px solid #ddd; text-align: left\">Pricing Model</th>\n<th style=\"padding: 12px; border: 1px solid #ddd; text-align: left\">Self-Hosting</th>\n<th style=\"padding: 12px; border: 1px solid #ddd; text-align: left\">Learning Curve</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\">n8n</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Technical teams, AI workflows</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Deep, native</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">400+</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Free/cloud/enterprise</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Yes</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Moderate-High</td>\n</tr>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Zapier</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Non-technical, SMBs, enterprise</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Strong, orchestration</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">8,000+</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Free/task-based</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">No</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Very Low</td>\n</tr>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Make</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Mixed teams, complex workflows</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Advanced, visual</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">2,000+</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Free/operation-based</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">No</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Moderate</td>\n</tr>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Power Automate</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Microsoft-centric enterprises</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">AI Builder, RPA</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">1,000+</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">User/bot/pay-as-you-go</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">No</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Moderate</td>\n</tr>\n<tr>\n<td style=\"padding: 12px; border: 1px solid #ddd\">ClickUp</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Teams, project management</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">AI Brain, agents</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">1,000+</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">User-based/add-ons</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">No</td>\n<td style=\"padding: 12px; border: 1px solid #ddd\">Low (AI/no-code)</td>\n</tr>\n</tbody>\n</table>\n<p>¬†<br/>\n¬†</p>\n<p><a href=\"https://abid.work\" rel=\"noopener\"><b><strong><a href=\"https://abid.work\" rel=\"noopener noreferrer\" target=\"_blank\">Abid Ali Awan</a></strong></b></a> (<a href=\"https://www.linkedin.com/in/1abidaliawan\" rel=\"noopener\">@1abidaliawan</a>) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in technology management and a bachelor's degree in telecommunication engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness.</p>\n</div><br/>\n<!--<hr class=\"grey-line\"><br>\t\n  <div><h3>Our Top 5 Free Course Recommendations</h3><br>\n\t  \t</div>\t\n-->\n<div class=\"box-form\">\n<!-- Mailchimp for WordPress v4.10.9 - https://wordpress.org/plugins/mailchimp-for-wp/ --><!-- / Mailchimp for WordPress Plugin --></div>\n<!-- You can start editing here. -->\n\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<br/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/the-data-detox-training-yourself-for-the-messy-noisy-real-world\" rel=\"prev\">&lt;= Previous post</a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/5-data-privacy-stories-from-2025-every-analyst-should-know\" rel=\"next\">Next post =&gt;</a></div>\n<br/><br/>\n</div>\n<!--#content end--></div>",
    "thumbnail": "https://www.kdnuggets.com/wp-content/uploads/awan_5_workflow_automation_tools_professionals_6.png",
    "word_count": 693,
    "scraped_at": "2025-12-17T18:52:50.833160"
  },
  {
    "title": "The Data Detox: Training Yourself for the Messy, Noisy, Real World",
    "url": "https://www.kdnuggets.com/the-data-detox-training-yourself-for-the-messy-noisy-real-world",
    "summary": "In this article, we‚Äôll use a real-life data project to explore four practical steps for preparing to deal with messy, real-life datasets.",
    "image_url": "https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_1.png",
    "published_date": null,
    "author": "https://www.facebook.com/kdnuggets",
    "full_text": "Image by Author\n#\nIntroduction\nWe have all spent hours debugging a model, only to discover that it wasn't the algorithm but a wrong null value manipulating your results in row 47,832. Kaggle competitions give the impression that data is produced as clean, well-labeled CSVs with no class imbalance issues, but in reality, that is not the case.\nIn this article, we‚Äôll use a real-life data project to explore four practical steps for preparing to deal with messy, real-life datasets.\n#\nNoBroker Data Project: A Hands-On Test of Real-World Chaos\nNoBroker is an Indian property technology (prop-tech) company that connects property owners and tenants directly in a broker-free marketplace.\nThis data project\nis used during the recruitment process for the data science positions at NoBroker.\nIn this data project, NoBroker wants you to build a predictive model that estimates how many interactions a property will receive within a given time frame. We won't complete the entire project here, but it‚Äôll help us discover methods for training ourselves on messy real-world data.\nIt has three datasets:\nproperty_data_set.csv\nContains property details such as type, location, amenities, size, rent, and other housing features.\nproperty_photos.tsv\nContains property photos.\nproperty_interactions.csv\nContains the timestamp of the interaction on the properties.\n#\nComparing Clean Interview Data Versus Real Production Data: The Reality Check\nInterview datasets are polished, balanced, and boring. Real production data? It's a dumpster fire with missing values, duplicate rows, inconsistent formats, and silent errors that wait until Friday at 5 PM to break your pipeline.\nTake the NoBroker property dataset, a real-world mess with 28,888 properties across three tables. At first glance, it looks fine. But dig deeper, and you'll find 11,022 missing photo uniform resource locators (URLs), corrupted JSON strings with rogue backslashes, and more.\nThis is the line between clean and chaotic. Clean data trains you to build models, but production data trains you to survive by struggling.\nWe‚Äôll explore four practices to train yourself.\n#\nPractice #1: Handling Missing Data\nMissing data isn't just annoying; it's a decision point. Delete the row? Fill it with the mean? Flag it as unknown? The answer depends on why the data is missing and how much you can afford to lose.\nThe NoBroker dataset had three types of missing data. The\nphoto_urls\ncolumn was missing 11,022 values out of 28,888 rows ‚Äî that is 38% of the dataset. Here is the code.\npics.isna().sum()\nHere is the output.\nDeleting these rows would wipe out valuable property records. Instead, the solution was to treat missing photos as if there were zero and move on.\ndef correction(x):\nif x is np.nan or x == 'NaN':\nreturn 0¬† # Missing photos = 0 photos\nelse:\nreturn len(json.loads(x.replace('\\\\', '').replace('{title','{\"title')))\npics['photo_count'] = pics['photo_urls'].apply(correction)\nFor numerical columns like\ntotal_floor\n(23 missing) and categorical columns like\nbuilding_type\n(38 missing), the strategy was imputation. Fill numerical gaps with the mean, and categorical gaps with the mode.\nfor col in x_remain_withNull.columns:\nx_remain[col] = x_remain_withNull[col].fillna(x_remain_withNull[col].mean())\nfor col in x_cat_withNull.columns:\nx_cat[col] = x_cat_withNull[col].fillna(x_cat_withNull[col].mode()[0])\nThe first decision: do not delete without a questioning mind!\nUnderstand the pattern. The missing photo URLs were not random.\n#\nPractice #2: Detecting Outliers\nAn outlier is not always an error, but it is always suspicious.\nCan you imagine a property with 21 bathrooms, 800 years old, or 40,000 square feet of space? You either found your dream place or someone made a data entry error.\nThe NoBroker dataset was full of these red flags. Box plots revealed extreme values across multiple columns: property ages over 100, sizes beyond 10,000 square feet (sq ft), and deposits exceeding 3.5 million. Some were legitimate luxury properties. Most were data entry mistakes.\ndf_num.plot(kind='box', subplots=True, figsize=(22,10))\nplt.show()\nHere is the output.\nThe solution was interquartile range (IQR)-based outlier removal, a simple statistical method that flags values beyond 2 times the IQR.\nTo handle this, we first write a function that removes those outliers.\ndef remove_outlier(df_in, col_name):\nq1 = df_in[col_name].quantile(0.25)\nq3 = df_in[col_name].quantile(0.75)\niqr = q3 - q1\nfence_low = q1 - 2 * iqr\nfence_high = q3 + 2 * iqr\ndf_out = df_in.loc[(df_in[col_name] <= fence_high) & (df_in[col_name] >= fence_low)]\nreturn df_out¬† # Note: Multiplier changed from 1.5 to 2 to match implementation.\nAnd we run this code on numerical columns.\ndf = dataset.copy()\nfor col in df_num.columns:\nif col in ['gym', 'lift', 'swimming_pool', 'request_day_within_3d', 'request_day_within_7d']:\ncontinue¬† # Skip binary and target columns\ndf = remove_outlier(df, col)\nprint(f\"Before: {dataset.shape[0]} rows\")\nprint(f\"After: {df.shape[0]} rows\")\nprint(f\"Removed: {dataset.shape[0] - df.shape[0]} rows ({((dataset.shape[0] - df.shape[0]) / dataset.shape[0] * 100):.1f}% reduction)\")\nHere is the output.\nAfter removing outliers, the dataset shrank from 17,386 rows to 15,170, losing 12.7% of the data while keeping the model sane. The trade-off was worth it.\nFor target variables like\nrequest_day_within_3d\n, capping was used instead of deletion. Values above 10 were capped at 10 to prevent extreme outliers from skewing predictions. In the following code, we also compare the results before and after.\ndef capping_for_3days(x):\nnum = 10\nreturn num if x > num else x\ndf['request_day_within_3d_capping'] = df['request_day_within_3d'].apply(capping_for_3days)\nbefore_count = (df['request_day_within_3d'] > 10).sum()\nafter_count = (df['request_day_within_3d_capping'] > 10).sum()\ntotal_rows = len(df)\nchange_count = before_count - after_count\npercent_change = (change_count / total_rows) * 100\nprint(f\"Before capping (>10): {before_count}\")\nprint(f\"After capping (>10): {after_count}\")\nprint(f\"Reduced by: {change_count} ({percent_change:.2f}% of total rows affected)\")\nThe result?\nA cleaner distribution, better model performance, and fewer debugging sessions.\n#\nPractice #3: Dealing with Duplicates and Inconsistencies\nDuplicates are easy. Inconsistencies are hard. A duplicate row is just\ndf.drop_duplicates()\n. An inconsistent format, like a JSON string that's been mangled by three different systems, requires detective work.\nThe NoBroker dataset had one of the worst JSON inconsistencies I've seen. The\nphoto_urls\ncolumn was supposed to contain valid JSON arrays, but instead, it was filled with malformed strings, missing quotes, escaped backslashes, and random trailing characters.\ntext_before = pics['photo_urls'][0]\nprint('Before Correction: \\n\\n', text_before)\nHere is the before correction.\nThe fix required multiple string replacements to correct the formatting before parsing. Here is the code.\ntext_after = text_before.replace('\\\\', '').replace('{title', '{\"title').replace(']\"', ']').replace('],\"', ']\",\"')\nparsed_json = json.loads(text_after)\nHere is the output.\nThe JSON was indeed valid and parseable after the fix. It is not the cleanest way to do this kind of string manipulation, but it works.\nYou see inconsistent formats everywhere: dates saved as strings, typos in categorical values, and numerical IDs saved as floats.\nThe solution is standardization, as we did with the JSON formatting.\n#\nPractice #4: Data Type Validation and Schema Checks\nIt all begins when you load your data. Finding out later that dates are strings or that numbers are objects would be a waste of time.\nIn the NoBroker project, the types were validated during the CSV read itself, as the project was enforcing the right data types upfront with\npandas\nparameters. Here is the code.\ndata = pd.read_csv('property_data_set.csv')\nprint(data['activation_date'].dtype)\ndata = pd.read_csv('property_data_set.csv',\nparse_dates=['activation_date'],\ninfer_datetime_format=True,\ndayfirst=True)\nprint(data['activation_date'].dtype)\nHere is the output.\nThe same validation was applied to the interaction dataset.\ninteraction = pd.read_csv('property_interactions.csv',\nparse_dates=['request_date'],\ninfer_datetime_format=True,\ndayfirst=True)\nNot only was this good practice, but it was essential for anything downstream. The project required calculations of date and time differences between the activation and request dates.\nSo the following code would produce an error if dates are strings.\nnum_req['request_day'] = (num_req['request_date'] - num_req['activation_date']) / np.timedelta64(1, 'D')\nSchema checks will ensure that the structure does not change, but in reality, the data will also drift as its distribution will tend to change over time. You can mimic this drift by having input proportions vary a little and check whether your model or its validation is able to detect and respond to that drift.\n#\nDocumenting Your Cleaning Steps\nIn three months, you won't remember why you limited\nrequest_day_within_3d\nto 10. Six months from now, your teammate will break the pipeline by removing your outlier filter. In a year, the model will hit production, and no one will understand why it simply fails.\nDocumentation isn't optional. That is the difference between a reproducible pipeline and a voodoo script that works until it doesn‚Äôt.\nThe NoBroker project documented every transformation in code comments and structured notebook sections with explanations and a table of contents.\n# Assignment\n# Read and Explore All Datasets\n# Data Engineering\nHandling Pics Data\nNumber of Interactions Within 3 Days\nNumber of Interactions Within 7 Days\nMerge Data\n# Exploratory Data Analysis and Processing\n# Feature Engineering\nRemove Outliers\nOne-Hot Encoding\nMinMaxScaler\nClassical Machine Learning\nPredicting Interactions Within 3 Days\nDeep Learning\n# Try to correct the first Json\n# Try to replace corrupted values then convert to json\n# Function to correct corrupted json and get count of photos\nVersion control matters too. Track changes to your cleaning logic. Save intermediate datasets. Keep a changelog of what you tried and what worked.\nThe goal isn't perfection. The goal is clarity. If you can't explain why you made a decision, you can't defend it when the model fails.\n#\nFinal Thoughts\nClean data is a myth. The best data scientists are not the ones who run away from messy datasets; they are the ones who know how to tame them. They discover the missing values before training.\nThey are able to identify the outliers before they influence predictions. They check schemas before joining tables. And they write everything down so that the next person doesn't have to begin from zero.\nNo real impact comes from perfect data. It comes from the ability to deal with erroneous data and still construct something functional.\nSo when you have to deal with a dataset and you see null values, broken strings, and outliers, don‚Äôt fear. What you see is not a problem but an opportunity to show your skills against a real-world dataset.\nNate Rosidi\nis a data scientist and in product strategy. He's also an adjunct professor teaching analytics, and is the founder of StrataScratch, a platform helping data scientists prepare for their interviews with real interview questions from top companies. Nate writes on the latest trends in the career market, gives interview advice, shares data science projects, and covers everything SQL.\n<= Previous post\nNext post =>",
    "full_html": "<div class=\"single\" id=\"content\">\n\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<p><center><img alt=\"Data Detox\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_2-scaled.png\" width=\"100%\"><br>\n<font size=\"-1\">Image by Author</font></br></img></center><br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Introduction</h2>\n<p>¬†<br/>\nWe have all spent hours debugging a model, only to discover that it wasn't the algorithm but a wrong null value manipulating your results in row 47,832. Kaggle competitions give the impression that data is produced as clean, well-labeled CSVs with no class imbalance issues, but in reality, that is not the case.</p><div class=\"kdnug-d80f9c469fded06ffe1ba32dbd80cb67 kdnug-after-first-paragraph\" id=\"kdnug-d80f9c469fded06ffe1ba32dbd80cb67\"></div>\n<p>In this article, we‚Äôll use a real-life data project to explore four practical steps for preparing to deal with messy, real-life datasets.</p><div class=\"kdnug-b5627f4fa196fba42a8501523cc2accf kdnug-in-content-1\" id=\"kdnug-b5627f4fa196fba42a8501523cc2accf\" style=\"padding-bottom:180px; padding-top:20px\"></div>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>NoBroker Data Project: A Hands-On Test of Real-World Chaos</h2>\n<p>¬†<br/>\nNoBroker is an Indian property technology (prop-tech) company that connects property owners and tenants directly in a broker-free marketplace.</p>\n<p>¬†<br/>\n<img alt=\"Data Detox\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_3-scaled.png\" width=\"100%\"><br/>\n¬†</img></p>\n<p><strong><a href=\"https://platform.stratascratch.com/data-projects/property-click-prediction?utm_source=blog&amp;utm_medium=click&amp;utm_campaign=kdn+data+detox\" target=\"_blank\">This data project</a></strong> is used during the recruitment process for the data science positions at NoBroker.</p>\n<p>In this data project, NoBroker wants you to build a predictive model that estimates how many interactions a property will receive within a given time frame. We won't complete the entire project here, but it‚Äôll help us discover methods for training ourselves on messy real-world data.</p>\n<p>It has three datasets:</p>\n<ul>\n<li><code>property_data_set.csv</code>\n<ul>\n<li>Contains property details such as type, location, amenities, size, rent, and other housing features.\n</li></ul>\n<li><code>property_photos.tsv</code>\n<ul>\n<li>Contains property photos.\n</li></ul>\n<li><code>property_interactions.csv</code>\n<ul>\n<li>Contains the timestamp of the interaction on the properties.\n</li></ul>\n</li></li></li></ul>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Comparing Clean Interview Data Versus Real Production Data: The Reality Check</h2>\n<p>¬†<br/>\nInterview datasets are polished, balanced, and boring. Real production data? It's a dumpster fire with missing values, duplicate rows, inconsistent formats, and silent errors that wait until Friday at 5 PM to break your pipeline.</p>\n<p>Take the NoBroker property dataset, a real-world mess with 28,888 properties across three tables. At first glance, it looks fine. But dig deeper, and you'll find 11,022 missing photo uniform resource locators (URLs), corrupted JSON strings with rogue backslashes, and more.</p>\n<p>This is the line between clean and chaotic. Clean data trains you to build models, but production data trains you to survive by struggling.</p>\n<p>We‚Äôll explore four practices to train yourself.</p>\n<p>¬†<br/>\n<img alt=\"Data Detox\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_4.png\" width=\"100%\"/><br/>\n¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Practice #1: Handling Missing Data</h2>\n<p>¬†<br/>\nMissing data isn't just annoying; it's a decision point. Delete the row? Fill it with the mean? Flag it as unknown? The answer depends on why the data is missing and how much you can afford to lose.</p>\n<p>The NoBroker dataset had three types of missing data. The <code>photo_urls</code> column was missing 11,022 values out of 28,888 rows ‚Äî that is 38% of the dataset. Here is the code.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>pics.isna().sum()</code></pre>\n</div>\n<p>¬†</p>\n<p>Here is the output.</p>\n<p>¬†<br/>\n<img alt=\"Data Detox\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_5.png\" width=\"100%\"/><br/>\n¬†</p>\n<p>Deleting these rows would wipe out valuable property records. Instead, the solution was to treat missing photos as if there were zero and move on.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>def correction(x):\r\n¬† ¬† if x is np.nan or x == 'NaN':\r\n¬† ¬† ¬† ¬† return 0¬† # Missing photos = 0 photos\r\n¬† ¬† else:\r\n¬† ¬† ¬† ¬† return len(json.loads(x.replace('\\\\', '').replace('{title','{\"title')))\r\npics['photo_count'] = pics['photo_urls'].apply(correction)</code></pre>\n</div>\n<p>¬†</p>\n<p>For numerical columns like <code>total_floor</code> (23 missing) and categorical columns like <code>building_type</code> (38 missing), the strategy was imputation. Fill numerical gaps with the mean, and categorical gaps with the mode.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>for col in x_remain_withNull.columns:\r\n¬† ¬† x_remain[col] = x_remain_withNull[col].fillna(x_remain_withNull[col].mean())\r\nfor col in x_cat_withNull.columns:\r\n¬† ¬† x_cat[col] = x_cat_withNull[col].fillna(x_cat_withNull[col].mode()[0])</code></pre>\n</div>\n<p>¬†</p>\n<p>The first decision: do not delete without a questioning mind!</p>\n<p>Understand the pattern. The missing photo URLs were not random.</p>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Practice #2: Detecting Outliers</h2>\n<p>¬†<br/>\nAn outlier is not always an error, but it is always suspicious.</p>\n<p>Can you imagine a property with 21 bathrooms, 800 years old, or 40,000 square feet of space? You either found your dream place or someone made a data entry error.</p>\n<p>The NoBroker dataset was full of these red flags. Box plots revealed extreme values across multiple columns: property ages over 100, sizes beyond 10,000 square feet (sq ft), and deposits exceeding 3.5 million. Some were legitimate luxury properties. Most were data entry mistakes.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>df_num.plot(kind='box', subplots=True, figsize=(22,10))\r\nplt.show()</code></pre>\n</div>\n<p>¬†</p>\n<p>Here is the output.</p>\n<p>¬†<br/>\n<img alt=\"Data Detox\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_6.png\" width=\"100%\"/><br/>\n¬†</p>\n<p>The solution was interquartile range (IQR)-based outlier removal, a simple statistical method that flags values beyond 2 times the IQR.</p>\n<p>To handle this, we first write a function that removes those outliers.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>def remove_outlier(df_in, col_name):\r\n¬† ¬† q1 = df_in[col_name].quantile(0.25)\r\n¬† ¬† q3 = df_in[col_name].quantile(0.75)\r\n¬† ¬† iqr = q3 - q1\r\n¬† ¬† fence_low = q1 - 2 * iqr\r\n¬† ¬† fence_high = q3 + 2 * iqr\r\n¬† ¬† df_out = df_in.loc[(df_in[col_name] &lt;= fence_high) &amp; (df_in[col_name] &gt;= fence_low)]\r\n¬† ¬† return df_out¬† # Note: Multiplier changed from 1.5 to 2 to match implementation.</code></pre>\n</div>\n<p>¬†</p>\n<p>And we run this code on numerical columns.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>df = dataset.copy()\r\nfor col in df_num.columns:\r\n¬† ¬† if col in ['gym', 'lift', 'swimming_pool', 'request_day_within_3d', 'request_day_within_7d']:\r\n¬† ¬† ¬† ¬† continue¬† # Skip binary and target columns\r\n¬† ¬† df = remove_outlier(df, col)\r\nprint(f\"Before: {dataset.shape[0]} rows\")\r\nprint(f\"After: {df.shape[0]} rows\")\r\nprint(f\"Removed: {dataset.shape[0] - df.shape[0]} rows ({((dataset.shape[0] - df.shape[0]) / dataset.shape[0] * 100):.1f}% reduction)\")</code></pre>\n</div>\n<p>¬†</p>\n<p>Here is the output.</p>\n<p>¬†<br/>\n<img alt=\"Data Detox\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_7.png\" width=\"100%\"/><br/>\n¬†</p>\n<p>After removing outliers, the dataset shrank from 17,386 rows to 15,170, losing 12.7% of the data while keeping the model sane. The trade-off was worth it.</p>\n<p>For target variables like <code>request_day_within_3d</code>, capping was used instead of deletion. Values above 10 were capped at 10 to prevent extreme outliers from skewing predictions. In the following code, we also compare the results before and after.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>def capping_for_3days(x):\r\n¬† ¬† num = 10\r\n¬† ¬† return num if x &gt; num else x\r\ndf['request_day_within_3d_capping'] = df['request_day_within_3d'].apply(capping_for_3days)\r\nbefore_count = (df['request_day_within_3d'] &gt; 10).sum()\r\nafter_count = (df['request_day_within_3d_capping'] &gt; 10).sum()\r\ntotal_rows = len(df)\r\nchange_count = before_count - after_count\r\npercent_change = (change_count / total_rows) * 100\r\nprint(f\"Before capping (&gt;10): {before_count}\")\r\nprint(f\"After capping (&gt;10): {after_count}\")\r\nprint(f\"Reduced by: {change_count} ({percent_change:.2f}% of total rows affected)\")</code></pre>\n</div>\n<p>¬†</p>\n<p>The result?</p>\n<p>¬†<br/>\n<img alt=\"Data Detox\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_8.png\" width=\"100%\"/><br/>\n¬†</p>\n<p>A cleaner distribution, better model performance, and fewer debugging sessions.</p>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Practice #3: Dealing with Duplicates and Inconsistencies</h2>\n<p>¬†<br/>\nDuplicates are easy. Inconsistencies are hard. A duplicate row is just <code>df.drop_duplicates()</code>. An inconsistent format, like a JSON string that's been mangled by three different systems, requires detective work.</p>\n<p>The NoBroker dataset had one of the worst JSON inconsistencies I've seen. The <code>photo_urls</code> column was supposed to contain valid JSON arrays, but instead, it was filled with malformed strings, missing quotes, escaped backslashes, and random trailing characters.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>text_before = pics['photo_urls'][0]\r\nprint('Before Correction: \\n\\n', text_before)</code></pre>\n</div>\n<p>¬†</p>\n<p>Here is the before correction.</p>\n<p>¬†<br/>\n<img alt=\"Data Detox\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_9.png\" width=\"100%\"/><br/>\n¬†</p>\n<p>The fix required multiple string replacements to correct the formatting before parsing. Here is the code.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>text_after = text_before.replace('\\\\', '').replace('{title', '{\"title').replace(']\"', ']').replace('],\"', ']\",\"')\r\nparsed_json = json.loads(text_after)</code></pre>\n</div>\n<p>¬†</p>\n<p>Here is the output.</p>\n<p>¬†<br/>\n<img alt=\"Data Detox\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_10.png\" width=\"100%\"/><br/>\n¬†</p>\n<p>The JSON was indeed valid and parseable after the fix. It is not the cleanest way to do this kind of string manipulation, but it works.</p>\n<p>You see inconsistent formats everywhere: dates saved as strings, typos in categorical values, and numerical IDs saved as floats.</p>\n<p>The solution is standardization, as we did with the JSON formatting.</p>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Practice #4: Data Type Validation and Schema Checks</h2>\n<p>¬†<br/>\nIt all begins when you load your data. Finding out later that dates are strings or that numbers are objects would be a waste of time.</p>\n<p>In the NoBroker project, the types were validated during the CSV read itself, as the project was enforcing the right data types upfront with <strong><a href=\"https://pandas.pydata.org/\" target=\"_blank\">pandas</a></strong> parameters. Here is the code.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>data = pd.read_csv('property_data_set.csv')\r\nprint(data['activation_date'].dtype)¬†¬†\r\ndata = pd.read_csv('property_data_set.csv',\r\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†parse_dates=['activation_date'],¬†\r\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†infer_datetime_format=True,¬†\r\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†dayfirst=True)\r\nprint(data['activation_date'].dtype)</code></pre>\n</div>\n<p>¬†</p>\n<p>Here is the output.</p>\n<p>¬†<br/>\n<img alt=\"Data Detox\" class=\"perfmatters-lazy article-img\" decoding=\"async\" src=\"https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_11.png\" width=\"100%\"/><br/>\n¬†</p>\n<p>The same validation was applied to the interaction dataset.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>interaction = pd.read_csv('property_interactions.csv',\r\n    parse_dates=['request_date'],¬†\r\n¬† ¬† infer_datetime_format=True,¬†\r\n¬† ¬† dayfirst=True)</code></pre>\n</div>\n<p>¬†</p>\n<p>Not only was this good practice, but it was essential for anything downstream. The project required calculations of date and time differences between the activation and request dates.</p>\n<p>So the following code would produce an error if dates are strings.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code>num_req['request_day'] = (num_req['request_date'] - num_req['activation_date']) / np.timedelta64(1, 'D')</code></pre>\n</div>\n<p>¬†</p>\n<p>Schema checks will ensure that the structure does not change, but in reality, the data will also drift as its distribution will tend to change over time. You can mimic this drift by having input proportions vary a little and check whether your model or its validation is able to detect and respond to that drift.</p>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Documenting Your Cleaning Steps</h2>\n<p>¬†<br/>\nIn three months, you won't remember why you limited <code>request_day_within_3d</code> to 10. Six months from now, your teammate will break the pipeline by removing your outlier filter. In a year, the model will hit production, and no one will understand why it simply fails.</p>\n<p>Documentation isn't optional. That is the difference between a reproducible pipeline and a voodoo script that works until it doesn‚Äôt.</p>\n<p>The NoBroker project documented every transformation in code comments and structured notebook sections with explanations and a table of contents.</p>\n<div style=\"width: 98%; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px\">\n<pre><code># Assignment\r\n# Read and Explore All Datasets\r\n# Data Engineering\r\nHandling Pics Data\r\nNumber of Interactions Within 3 Days\r\nNumber of Interactions Within 7 Days\r\nMerge Data\r\n# Exploratory Data Analysis and Processing\r\n# Feature Engineering\r\nRemove Outliers\r\nOne-Hot Encoding\r\nMinMaxScaler\r\nClassical Machine Learning\r\nPredicting Interactions Within 3 Days\r\nDeep Learning\r\n# Try to correct the first Json\r\n# Try to replace corrupted values then convert to json\r\n# Function to correct corrupted json and get count of photos</code></pre>\n</div>\n<p>¬†</p>\n<p>Version control matters too. Track changes to your cleaning logic. Save intermediate datasets. Keep a changelog of what you tried and what worked.</p>\n<p>The goal isn't perfection. The goal is clarity. If you can't explain why you made a decision, you can't defend it when the model fails.</p>\n<p>¬†</p>\n<h2><font color=\"#f3ac35\">#¬†</font>Final Thoughts</h2>\n<p>¬†<br/>\nClean data is a myth. The best data scientists are not the ones who run away from messy datasets; they are the ones who know how to tame them. They discover the missing values before training.</p>\n<p>They are able to identify the outliers before they influence predictions. They check schemas before joining tables. And they write everything down so that the next person doesn't have to begin from zero.</p>\n<p>No real impact comes from perfect data. It comes from the ability to deal with erroneous data and still construct something functional.</p>\n<p>So when you have to deal with a dataset and you see null values, broken strings, and outliers, don‚Äôt fear. What you see is not a problem but an opportunity to show your skills against a real-world dataset.<br/>\n¬†<br/>\n¬†</p>\n<p><a href=\"https://twitter.com/StrataScratch\" rel=\"noopener\"><b><strong><a href=\"https://twitter.com/StrataScratch\" rel=\"noopener noreferrer\" target=\"_blank\">Nate Rosidi</a></strong></b></a> is a data scientist and in product strategy. He's also an adjunct professor teaching analytics, and is the founder of StrataScratch, a platform helping data scientists prepare for their interviews with real interview questions from top companies. Nate writes on the latest trends in the career market, gives interview advice, shares data science projects, and covers everything SQL.</p>\n</div><br/>\n<!--<hr class=\"grey-line\"><br>\t\n  <div><h3>Our Top 5 Free Course Recommendations</h3><br>\n\t  \t</div>\t\n-->\n<div class=\"box-form\">\n<!-- Mailchimp for WordPress v4.10.9 - https://wordpress.org/plugins/mailchimp-for-wp/ --><!-- / Mailchimp for WordPress Plugin --></div>\n<!-- You can start editing here. -->\n\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<br/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/how-transformers-think-the-information-flow-that-makes-language-models-work\" rel=\"prev\">&lt;= Previous post</a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/5-workflow-automation-tools-for-all-professionals\" rel=\"next\">Next post =&gt;</a></div>\n<br/><br/>\n</div>\n<!--#content end--></div>",
    "thumbnail": "https://www.kdnuggets.com/wp-content/uploads/Rosidi_The_Data_Detox_1.png",
    "word_count": 1675,
    "scraped_at": "2025-12-17T18:52:51.123811"
  }
]